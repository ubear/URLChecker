# 检测页面框架

### 一个题目

请设计一个系统，自动完成对于某网站系统可靠性的检测。具体要求：
- 定时递归检测所有XXX域名的页面以及这些页面上的链接的可达性，即有没有出现不可访问情况
- XXX域名页面很多，从各个方面考虑性能优化
- 对于错误的链接记录到日志中，日志包括：连接，时间，错误状态等。
- 考虑多线程的方式实现

### 想法

看到这个题目时，感觉这个东西可以做成一个类似于框架的实现。可以实现以下几点要求：
- 自定义检测的域名，并保留日志到以域名为特征的文件夹中
- 可以定义多个检测的域名
- 可以定时检测，日志文件名以时间作为名字
- 自定义页面爬取规则，因有的还需要检测js/css/image等文件的可访问状态
- 自定义过滤规则，因为有的不仅要检测某个域名，还需要检测子域名的页面


### 实现

主要部分在`worker.py`文件中。设计了两个类：
- `CheckUrl`:日志配置、线程池、页面爬取、过滤函数
- `MetaThreading`：单个线程运行、队列维护、重复排除

### 使用

#### 重写函数

在使用过程中，通过继承`CheckUrl`类，重写其`exctract_url(url)`和`url_filter(url)`两个函数。

第一个是提取页面函数，返回函数为`list`类型的数据，内容为页面的url。默认是使用`BeautifulSoup`库，要改写的话，可以使用`xpath`,`re`等库来提取url。

第二个函数是考虑到有些页面(html)需要打开，而有些url只需要检测是否可达(如css/js/image等链接)。在需要打开的页面中，要过滤掉一些不属于该域名下的页面。默认写的是凡是不属于初始化域名中的链接，都会过滤掉，即返回`False`，如果不需要被过滤，则返回`True`。

#### 配置

配置`config.py`文件中需要配置被检测域名、日志存储路径和相关格式、线程的个数、总共要检测的url个数等。